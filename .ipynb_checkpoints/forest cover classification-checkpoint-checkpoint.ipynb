{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # Cover Type Classification\n",
    "   \n",
    "   ### Objective: Build a deep learning model to predict the forest cover type from different cartographic variables.\n",
    "   ### Given: \n",
    "       1. Cover Types: ['Spruce/Fir', 'Lodgepole Pine','Ponderosa Pine', 'Cottonwood/Willow','Aspen', 'Douglas-fir', 'Krummholz']\n",
    "       2. A csv file ('cover_data.csv') that contains 581012 observations. Each observation has 55 columns (54 features and the last one being the class).\n",
    "   ### Assumption(s):\n",
    "       1. There are no separate test dataset. So, one must hold-out a small percentage of given input as test data.\n",
    "       2. There is no information about the use of predictions. Hence, we do not know how what to focus on (precision or recall). Generally, it's a good idea to have both scores 'high'.\n",
    "   ### Expected output: \n",
    "       1. A good model.\n",
    "       2. Model performance over epochs (accuracy, loss plots)\n",
    "       3. Some classification metrics (heatmap of confusion-matrix, classification-report etc).\n",
    "       4. Conclusions, thoughts and ways to improve classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydot in c:\\python310\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in c:\\python310\\lib\\site-packages (from pydot) (3.0.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "!pip install pydot\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, Dense, Dropout\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy, AUC, Accuracy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31072\\1885399775.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Disable those annoying warnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_logger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetLevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ERROR'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Turn off GPU usage for tf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'CUDA_VISIBLE_DEVICES'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'-1'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# Disable those annoying warnings\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# Turn off GPU usage for tf\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv('cover_data.csv')\n",
    "raw_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_df.hist(figsize=(30,30),column=raw_df.drop(columns=['class']).columns,color='purple',layout=(10,6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = raw_df.groupby(raw_df['class']).size()\n",
    "print(Counter(raw_df[\"class\"]))\n",
    "ax = result.plot(kind='bar',color='purple',title='Forest Classification')\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correlation = round(raw_df.corr(),4)\n",
    "correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50,30))\n",
    "sns.heatmap(correlation, annot=True)\n",
    "plt.show()\n",
    "\n",
    "# Loop over bottom diagonal of correlation matrix\n",
    "for i in range(len(correlation.columns)):\n",
    "    for j in range(i):\n",
    "        # Print variables with high correlation\n",
    "        if abs(correlation.iloc[i, j]) > 0.7:\n",
    "            print(correlation.columns[i], correlation.columns[j], correlation.iloc[i, j])\n",
    "else:\n",
    "    print('The coefficient of other features not greater than 0.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correlation_features = raw_df.corr()\n",
    "correlation_target =  correlation_features[['class']].drop(labels=['class'])\n",
    "print(correlation_target)\n",
    "plt.figure(figsize=(5,15))\n",
    "sns.heatmap(correlation_target, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(raw_df, features, label):\n",
    "    \"\"\"\n",
    "    Prepare data that can be readily consumed by ML/DL algorithms.\n",
    "    - separate features from class variables\n",
    "    - split into training and testing dataset\n",
    "    - scale numerical data\n",
    "    \n",
    "    param: a dataframe of input data\n",
    "    output: X_train_normalized, X_test_normalized, y_train, y_test\n",
    "    \"\"\"\n",
    "    X = raw_df[features]\n",
    "    y = raw_df[label]\n",
    "\n",
    "    # Split into train and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)\n",
    "\n",
    "    # normalize data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_normalized = scaler.fit_transform(X_train)\n",
    "    X_test_normalized = scaler.transform(X_test)\n",
    "\n",
    "    return X_train_normalized, X_test_normalized, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_features):\n",
    "    \"\"\"\n",
    "    Build the model architecture (and compile it).\n",
    "    input: number of features\n",
    "    output: Keras model object.\n",
    "    \"\"\"\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(256, input_dim=num_features, activation='relu'))\n",
    "#     classifier.add(Dropout(0.3))\n",
    "    classifier.add(Dense(128, activation='relu'))\n",
    "#     classifier.add(layers.Dropout(0.3))\n",
    "    classifier.add(Dense(64, activation='softmax'))\n",
    "    classifier.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# Plot the model\n",
    "    plot_model(classifier, to_file='classifier.png', show_shapes=True)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(class_names, y_pred, y_test):\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    heatmap = sns.heatmap(cm, fmt='g', cmap='Blues', annot=True, ax=ax)\n",
    "    ax.set_xlabel('Predicted class')\n",
    "    ax.set_ylabel('True class')\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    ax.xaxis.set_ticklabels(class_names)\n",
    "    ax.yaxis.set_ticklabels(class_names)\n",
    "    plt.show()\n",
    "    # Save the heatmap to file\n",
    "#     heatmapfig = heatmap.get_figure()\n",
    "#     heatmapfig.savefig(f'../output/confusion_matrix.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, param):\n",
    "\n",
    "    if param == 'acc':\n",
    "        plt.plot(history.history['accuracy'])\n",
    "        plt.plot(history.history['val_accuracy'])\n",
    "        plt.title('Model Accuracy')\n",
    "        plt.grid()\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Training', 'Validation'], loc='upper left')\n",
    "        plt.show()\n",
    "    elif param == 'loss':\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('Model Loss')\n",
    "        plt.grid()\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Training', 'Validation'], loc='upper right')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The main function below drives the entire code. It prepares the dataset, builds a model with appropriate parameters, evaluates the model and predicts on the test data. Finally, plots some performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    # EDA\n",
    "    # Uncomment the below two lines to run EDA. Takes > 10 min to run.\n",
    "    #my_report = sv.analyze(raw_df)\n",
    "    #my_report.show_html()\n",
    "    raw_df = pd.read_csv('cover_data.csv')\n",
    "    cols = raw_df.drop(columns=['Hillshade_9am','Wilderness_Area3','Soil_Type7','Soil_Type8','Soil_Type9',\n",
    "                                'Soil_Type15','Soil_Type16','Soil_Type16','Soil_Type25','Soil_Type26','Soil_Type28',\n",
    "                                 'Soil_Type34']).columns.tolist()\n",
    "    \n",
    "    features, label = cols[:-1], cols[-1]\n",
    "    X_train, X_test, y_train, y_test = prep_data(raw_df,features,label)\n",
    "\n",
    "    # Build a DL model\n",
    "    num_features = len(features)\n",
    "    model = build_model(num_features)\n",
    "\n",
    "    print(\"Summary report of Keras classifier:\")\n",
    "    model.summary()\n",
    "\n",
    "    num_epochs = 100\n",
    "    batch_size = 1024\n",
    "    earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=3)\n",
    "    history = model.fit(X_train,\n",
    "                        y_train,\n",
    "                        epochs=num_epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[earlystop_callback],\n",
    "                        validation_split=0.1,\n",
    "                        verbose=1)\n",
    "\n",
    "    plot_history(history, 'acc')\n",
    "    plot_history(history, 'loss')\n",
    "\n",
    "    score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f'Test loss: {round(score[0]*100,4)} %')\n",
    "    print(f'Test accuracy: {round(score[1]*100,4)} %')\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Convert the pred to discrete values\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    class_names = ['Spruce/Fir', 'Lodgepole Pine',\n",
    "                   'Ponderosa Pine', 'Cottonwood/Willow',\n",
    "                   'Aspen', 'Douglas-fir', 'Krummholz']\n",
    "    print(classification_report(y_test, y_pred, target_names=class_names))\n",
    "    plot_heatmap(class_names, y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions: \n",
    "The numbers along the diagonal of the heatmap show how many were correctly classified. All other numbers on either sides of the diagonal show mis-classifications. We see that Lodgepole Pine, Cottonwood Willow, Aspen, and Douglas-Fir suffer from a high percentage of mis-classifications. To investigate the possible causes, one can explore the following:\n",
    "\n",
    "1. Check the proportion of observations for each cover-type. Imbalances in the dataset will affect classification.\n",
    "2. How each wilderness area is distributed.\n",
    "3. Find the similarties among different cover-types (correlation, scatter-plots, etc.) These similarities might be one of the reasons the model might be tripping-up. There are ways to address it - one of it is to carefully remove all of the collinear variables, leaving only one.\n",
    "4. Remove noise, inconsistent data and errors in training data - this should be done carefully with domain experts.\n",
    "5. Try to use some other performance metric other than 'accuracy'. It fails to be a reliable metric when data in imbalanced. That is why we have other metrics such as Precision/Recall, F1-score etc.\n",
    "6. Try resampling the data (undersample or oversample appropriately or stratified). Downsampling could be done with thresholding.\n",
    "\n",
    "The most important thing to understand here is that in deep-learning, the gradient(s) of the majority class(es) dominate(s) and will influence the weight-updates. There are also some advanced techniques that will ameliorate this situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
